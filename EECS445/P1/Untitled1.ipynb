{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5545b749-db43-42ea-9cc3-86089b287593",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"EECS 445 - Winter 2023.\n",
    "\n",
    "Project 1\n",
    "\"\"\"\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import itertools\n",
    "import string\n",
    "\n",
    "from sklearn.svm import SVC, LinearSVC\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn import metrics\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "\n",
    "from helper import *\n",
    "\n",
    "import warnings\n",
    "from sklearn.exceptions import ConvergenceWarning\n",
    "\n",
    "warnings.simplefilter(action=\"ignore\", category=FutureWarning)\n",
    "warnings.simplefilter(action=\"ignore\", category=ConvergenceWarning)\n",
    "\n",
    "np.random.seed(445)\n",
    "\n",
    "\n",
    "\n",
    "def extract_word(input_string):\n",
    "    \"\"\"Preprocess review into list of tokens.\n",
    "\n",
    "    Convert input string to lowercase, replace punctuation with spaces, and split along whitespace.\n",
    "    Return the resulting array.\n",
    "\n",
    "    E.g.\n",
    "    > extract_word(\"I love EECS 445. It's my favorite course!\")\n",
    "    > [\"i\", \"love\", \"eecs\", \"445\", \"it\", \"s\", \"my\", \"favorite\", \"course\"]\n",
    "\n",
    "    Input:\n",
    "        input_string: text for a single review\n",
    "    Returns:\n",
    "        a list of words, extracted and preprocessed according to the directions\n",
    "        above.\n",
    "    \"\"\"\n",
    "    for i in input_string:\n",
    "        if i in string.punctuation:\n",
    "            temp = \"\"\n",
    "            temp += i\n",
    "            input_string = input_string.replace(temp, \"\")\n",
    "    return input_string.split()\n",
    "    # TODO: test\n",
    "\n",
    "\n",
    "def extract_dictionary(df):\n",
    "    \"\"\"Map words to index.\n",
    "\n",
    "    Reads a pandas dataframe, and returns a dictionary of distinct words\n",
    "    mapping from each distinct word to its index (ordered by when it was\n",
    "    found).\n",
    "\n",
    "    E.g., with input:\n",
    "        | text                          | label | ... |\n",
    "        | It was the best of times.     |  1    | ... |\n",
    "        | It was the blurst of times.   | -1    | ... |\n",
    "\n",
    "    The output should be a dictionary of indices ordered by first occurence in\n",
    "    the entire dataset:\n",
    "        {\n",
    "           it: 0,\n",
    "           was: 1,\n",
    "           the: 2,\n",
    "           best: 3,\n",
    "           of: 4,\n",
    "           times: 5,\n",
    "           blurst: 6\n",
    "        }\n",
    "    The index should be autoincrementing, starting at 0.\n",
    "\n",
    "    Input:\n",
    "        df: dataframe/output of load_data()\n",
    "    Returns:\n",
    "        a dictionary mapping words to an index\n",
    "    \"\"\"\n",
    "    word_dict = {}\n",
    "    n = len(df)\n",
    "    for i in range(n):\n",
    "        curr_row = df['text'][i]\n",
    "        curr_row = extract_word(curr_row)\n",
    "        for j in curr_row:\n",
    "            if j in word_dict:\n",
    "                word_dict.update({j: word_dict.get(j) + 1})\n",
    "            if j not in word_dict:\n",
    "                word_dict[j] = 1  \n",
    "    return word_dict\n",
    "\n",
    "\n",
    "def generate_feature_matrix(df, word_dict):\n",
    "    \"\"\"Create matrix of feature vectors for dataset.\n",
    "\n",
    "    Reads a dataframe and the dictionary of unique words to generate a matrix\n",
    "    of {1, 0} feature vectors for each review.  Use the word_dict to find the\n",
    "    correct index to set to 1 for each place in the feature vector. The\n",
    "    resulting feature matrix should be of dimension (# of reviews, # of words\n",
    "    in dictionary).\n",
    "\n",
    "    Input:\n",
    "        df: dataframe that has the text and labels\n",
    "        word_dict: dictionary of words mapping to indices\n",
    "    Returns:\n",
    "        a numpy matrix of dimension (# of reviews, # of words in dictionary)\n",
    "    \"\"\"\n",
    "    number_of_reviews = df.shape[0]\n",
    "    number_of_words = len(word_dict)\n",
    "    feature_matrix = np.zeros((number_of_reviews, number_of_words))\n",
    "    for i in range(number_of_reviews):\n",
    "        curr_rev = extract_word(df['text'][i])\n",
    "        for j in range(len(word_dict)):\n",
    "            if list(word_dict)[j] in curr_rev:\n",
    "                feature_matrix[i][j] = 1\n",
    "    return feature_matrix\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def performance(y_true, y_pred, metric=\"accuracy\"):\n",
    "    \"\"\"Calculate performance metrics.\n",
    "\n",
    "    Performance metrics are evaluated on the true labels y_true versus the\n",
    "    predicted labels y_pred.\n",
    "\n",
    "    Input:\n",
    "        y_true: (n,) array containing known labels\n",
    "        y_pred: (n,) array containing predicted scores\n",
    "        metric: string specifying the performance metric (default='accuracy'\n",
    "                 other options: 'f1-score', 'auroc', 'precision', 'sensitivity',\n",
    "                 and 'specificity')\n",
    "    Returns:\n",
    "        the performance as an np.float64\n",
    "    \"\"\"\n",
    "    cv = metrics.confusion_matrix(ytrue,ypred)\n",
    "    tp = cv[0,0]\n",
    "    fp = cv[0,1]\n",
    "    fn = cv[1,0]\n",
    "    tn = cv[1,1]\n",
    "    if metric == \"accuracy\":\n",
    "        if tp+fp+fn+tn == 0:\n",
    "            return 0\n",
    "        else:\n",
    "            return (tp+tn)/(tp+tn+fp+fn)\n",
    "    if metric == \"precision\":\n",
    "        if tp+fp == 0:\n",
    "            return 0\n",
    "        else:\n",
    "            return tp/(tp+fp)\n",
    "    if metric == \"sensitivity\":\n",
    "        if tp+fn == 0:\n",
    "            return 0\n",
    "        else:\n",
    "            return tp/(tp+fn)\n",
    "    if metric == \"specificity\":\n",
    "        if tn+fp == 0:\n",
    "            return 0\n",
    "        else:\n",
    "            return tn/(tn+fp)\n",
    "    if metric == \"f1-score\":\n",
    "        if tp+fn == 0 or tn+fp == 0:\n",
    "            return 0\n",
    "        if tp/(tp+fn) + tn/(tn+fp) == 0:\n",
    "            return 0\n",
    "        else:\n",
    "            return 2*(tp/(tp+fn))*(tn/(tn+fp))/(tp/(tp+fn) + tn/(tn+fp))\n",
    "    else:\n",
    "        return 0\n",
    "    \n",
    "    # TODO: Implement this function\n",
    "    # This is an optional but very useful function to implement.\n",
    "    # See the sklearn.metrics documentation for pointers on how to implement\n",
    "    # the requested metrics.\n",
    "\n",
    "\n",
    "def cv_performance(clf, X, y, k=5, metric=\"accuracy\"):\n",
    "    \"\"\"Split data into k folds and run cross-validation.\n",
    "\n",
    "    Splits the data X and the labels y into k-folds and runs k-fold\n",
    "    cross-validation: for each fold i in 1...k, trains a classifier on\n",
    "    all the data except the ith fold, and tests on the ith fold.\n",
    "    Calculates and returns the k-fold cross-validation performance metric for\n",
    "    classifier clf by averaging the performance across folds.\n",
    "    Input:\n",
    "        clf: an instance of SVC()\n",
    "        X: (n,d) array of feature vectors, where n is the number of examples\n",
    "           and d is the number of features\n",
    "        y: (n,) array of binary labels {1,-1}\n",
    "        k: an int specifying the number of folds (default=5)\n",
    "        metric: string specifying the performance metric (default='accuracy'\n",
    "             other options: 'f1-score', 'auroc', 'precision', 'sensitivity',\n",
    "             and 'specificity')\n",
    "    Returns:\n",
    "        average 'test' performance across the k folds as np.float64\n",
    "    \"\"\"\n",
    "    # TODO: Implement this function\n",
    "    # HINT: You may find the StratifiedKFold from sklearn.model_selection\n",
    "    # to be useful\n",
    "\n",
    "    # Put the performance of the model on each fold in the scores array\n",
    "    scores = []\n",
    "    skf = StratifiedKFold(n_splits=k)\n",
    "    for i, (train_index, test_index) in enumerate(skf.split(X, y)):\n",
    "        xtrain = X.iloc[train_index]\n",
    "        ytrain = y.iloc[train_index]\n",
    "        ytrue = y.iloc[test_index]\n",
    "        clf.fit(xtrain, ytrain)\n",
    "        if metric == \"AUROC\":\n",
    "            ypred = clf.decision_function(X.iloc[test_index])\n",
    "            scores.append(metrics.roc_auc_score(ytrue,ypred))\n",
    "        else:\n",
    "            ypred = clf.predict(X.iloc[test_index])\n",
    "            scores.append(performance(ytrue,ypred,metric))\n",
    "    return np.array(scores).mean()\n",
    "\n",
    "\n",
    "def select_param_linear(X, y, k=5, metric=\"accuracy\", C_range=[], loss=\"hinge\", \n",
    "                        penalty=\"l2\", dual=True):\n",
    "    \"\"\"Search for hyperparameters from the given candidates of linear SVM with \n",
    "    best k-fold CV performance.\n",
    "\n",
    "    Sweeps different settings for the hyperparameter of a linear-kernel SVM,\n",
    "    calculating the k-fold CV performance for each setting on X, y.\n",
    "    Input:\n",
    "        X: (n,d) array of feature vectors, where n is the number of examples\n",
    "        and d is the number of features\n",
    "        y: (n,) array of binary labels {1,-1}\n",
    "        k: int specifying the number of folds (default=5)\n",
    "        metric: string specifying the performance metric (default='accuracy',\n",
    "             other options: 'f1-score', 'auroc', 'precision', 'sensitivity',\n",
    "             and 'specificity')\n",
    "        C_range: an array with C values to be searched over\n",
    "        loss: string specifying the loss function used (default=\"hinge\",\n",
    "             other option of \"squared_hinge\")\n",
    "        penalty: string specifying the penalty type used (default=\"l2\",\n",
    "             other option of \"l1\")\n",
    "        dual: boolean specifying whether to use the dual formulation of the\n",
    "             linear SVM (set True for penalty \"l2\" and False for penalty \"l1\"ß)\n",
    "    Returns:\n",
    "        the parameter value for a linear-kernel SVM that maximizes the\n",
    "        average 5-fold CV performance.\n",
    "    \"\"\"\n",
    "    maxsofar = 0\n",
    "    for i in C_range:\n",
    "        clf = LinearSVC(penalty=penalty, loss=loss, dual=dual, C=i, random_state=445)\n",
    "        curperf = cv_performance(clf,X,y,k,metric)\n",
    "        if curperf > maxsofar:\n",
    "            maxsofar = curperf\n",
    "            maxi = i\n",
    "    return maxi\n",
    "        \n",
    "    # TODO: Implement this function\n",
    "    # HINT: You should be using your cv_performance function here\n",
    "    # to evaluate the performance of each SVM\n",
    "\n",
    "\n",
    "def plot_weight(X, y, penalty, C_range, loss, dual):\n",
    "    \"\"\"Create a plot of the L0 norm learned by a classifier for each C in C_range.\n",
    "\n",
    "    Input:\n",
    "        X: (n,d) array of feature vectors, where n is the number of examples\n",
    "        and d is the number of features\n",
    "        y: (n,) array of binary labels {1,-1}\n",
    "        penalty: string for penalty type to be forwarded to the LinearSVC constructor\n",
    "        C_range: list of C values to train a classifier on\n",
    "        loss: string for loss function to be forwarded to the LinearSVC constructor\n",
    "        dual: whether to solve the dual or primal optimization problem, to be\n",
    "            forwarded to the LinearSVC constructor\n",
    "    Returns: None\n",
    "        Saves a plot of the L0 norms to the filesystem.\n",
    "    \"\"\"\n",
    "    norm0 = []\n",
    "    # TODO: Implement this part of the function\n",
    "    # Here, for each value of c in C_range, you should\n",
    "    # append to norm0 the L0-norm of the theta vector that is learned\n",
    "    # when fitting an L2- or L1-penalty, degree=1 SVM to the data (X, y)\n",
    "\n",
    "    plt.plot(C_range, norm0)\n",
    "    plt.xscale(\"log\")\n",
    "    plt.legend([\"L0-norm\"])\n",
    "    plt.xlabel(\"Value of C\")\n",
    "    plt.ylabel(\"Norm of theta\")\n",
    "    plt.title(\"Norm-\" + penalty + \"_penalty.png\")\n",
    "    plt.savefig(\"Norm-\" + penalty + \"_penalty.png\")\n",
    "    plt.close()\n",
    "\n",
    "\n",
    "def select_param_quadratic(X, y, k=5, metric=\"accuracy\", param_range=[]):\n",
    "    \"\"\"Search for hyperparameters from the given candidates of quadratic SVM \n",
    "    with best k-fold CV performance.\n",
    "\n",
    "    Sweeps different settings for the hyperparameters of an quadratic-kernel SVM,\n",
    "    calculating the k-fold CV performance for each setting on X, y.\n",
    "    Input:\n",
    "        X: (n,d) array of feature vectors, where n is the number of examples\n",
    "           and d is the number of features\n",
    "        y: (n,) array of binary labels {1,-1}\n",
    "        k: an int specifying the number of folds (default=5)\n",
    "        metric: string specifying the performance metric (default='accuracy'\n",
    "                 other options: 'f1-score', 'auroc', 'precision', 'sensitivity',\n",
    "                 and 'specificity')\n",
    "        param_range: a (num_param, 2)-sized array containing the\n",
    "            parameter values to search over. The first column should\n",
    "            represent the values for C, and the second column should\n",
    "            represent the values for r. Each row of this array thus\n",
    "            represents a pair of parameters to be tried together.\n",
    "    Returns:\n",
    "        The parameter values for a quadratic-kernel SVM that maximize\n",
    "        the average 5-fold CV performance as a pair (C,r)\n",
    "    \"\"\"\n",
    "    # TODO: Implement this function\n",
    "    # Hint: This will be very similar to select_param_linear, except\n",
    "    # the type of SVM model you are using will be different...\n",
    "    best_C_val, best_r_val = 0.0, 0.0\n",
    "    return best_C_val, best_r_val\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def main():\n",
    "    # Read binary data\n",
    "    # NOTE: READING IN THE DATA WILL NOT WORK UNTIL YOU HAVE FINISHED\n",
    "    #       IMPLEMENTING generate_feature_matrix AND extract_dictionary\n",
    "    X_train, Y_train, X_test, Y_test, dictionary_binary = get_split_binary_data(\n",
    "        fname=\"data/dataset.csv\"\n",
    "    )\n",
    "    IMB_features, IMB_labels, IMB_test_features, IMB_test_labels = get_imbalanced_data(\n",
    "        dictionary_binary, fname=\"data/dataset.csv\"\n",
    "    )\n",
    "\n",
    "\n",
    "    # TODO: Questions 3, 4, 5\n",
    "\n",
    "    # Read multiclass data\n",
    "    # TODO: Question 6: Apply a classifier to heldout features, and then use\n",
    "    #       generate_challenge_labels to print the predicted labels\n",
    "    \n",
    "    (multiclass_features,\n",
    "    multiclass_labels,\n",
    "    multiclass_dictionary) = get_multiclass_training_data()\n",
    "    \n",
    "    heldout_features = get_heldout_reviews(multiclass_dictionary)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3955bb19-aeee-4bd0-a3d1-a1ab44a80055",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
